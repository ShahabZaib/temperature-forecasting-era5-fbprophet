{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0449247",
   "metadata": {},
   "source": [
    "## ðŸ”€ Merge Subset Files for Model Training\n",
    "\n",
    "This notebook loads all preprocessed yearly station CSV files and merges them into a unified dataset.\n",
    "\n",
    "ðŸ“Œ Steps:\n",
    "1. Loops over 242 station files (already subset by year)\n",
    "2. Cleans unnecessary columns (e.g., `expver`, `sst`)\n",
    "3. Concatenates all into a single DataFrame\n",
    "4. Saves the final merged output for model training\n",
    "\n",
    "âœ… Run this after all yearly subsets are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdab3e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "colums_to_drop = ['expver_x', 'number_x', 'sst', 'expver_y', 'number_y']\n",
    "for i in range(242):  # 0 to 241\n",
    "    df1 = pd.read_csv(f'Pakistan_data_1_points_2025_{i}.csv')\n",
    "    df2 = pd.read_csv(f'Pakistan_data_2_points_2025_{i}.csv')\n",
    "    \n",
    "    # Merging on common columns\n",
    "    merged_df = pd.merge(df1, df2, on=['latitude', 'longitude', 'valid_time'], how='inner')\n",
    "    merged_df = merged_df.drop(columns=colums_to_drop, axis=1)\n",
    "    # change the column names to be more informative\n",
    "    merged_df.rename(columns = {'valid_time': 'Date','latitude': 'Latitude', 'longitude': 'Longitude', 'u10':'Wind_u',\n",
    "                                'v10':'Wind_v', 'd2m':'Dewpoint_temperature', 't2m':'Temperature',\n",
    "                                'msl':'Air_pressure_at_mean_sea_level', 'sp':'Surface_pressure','tcc':'Total_cloud_cover',\n",
    "                                'tp':'Total_precipitation', 'e':'Evaporation', 'ro':'Runoff', 'sf':'Snowfall'}, inplace = True)\n",
    "    # Save each merged dataset separately\n",
    "    merged_df.to_csv(f'2025/Pakistan_merged_data_points_2025_cleaned_{i}.csv')\n",
    "\n",
    "print(\"Merging completed for all files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2a2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# create 8 variable names as data_2015 to data_2024 without looping\n",
    "data_2024_ = {}\n",
    "data_2023_ = {}\n",
    "data_2022_ = {}\n",
    "data_2021_ = {}\n",
    "data_2020_ = {}\n",
    "data_2019_ = {}\n",
    "data_2018_ = {}\n",
    "data_2017_ = {}\n",
    "data_2016_ = {}\n",
    "data_2015_ = {}\n",
    "\n",
    "# Change the path to match your folder structure and file names just dont _{i}.csv\n",
    "# Example data_2024_[i] = pd.read_csv(f'folder/file name_{i}.csv')\n",
    "\n",
    "for i in range (242):\n",
    "    data_2024_[i] = pd.read_csv(f'2024/Pakistan_merged_data_points_2024_cleaned_{i}.csv')\n",
    "    data_2023_[i] = pd.read_csv(f'2023/Pakistan_merged_data_points_2023_cleaned_{i}.csv')\n",
    "    data_2022_[i] = pd.read_csv(f'2022/Pakistan_merged_data_points_2022_cleaned_{i}.csv')\n",
    "    data_2021_[i] = pd.read_csv(f'2021/Pakistan_merged_data_points_2021_cleaned_{i}.csv')\n",
    "    data_2020_[i] = pd.read_csv(f'2020/Pakistan_merged_data_points_2020_cleaned_{i}.csv')\n",
    "    data_2019_[i] = pd.read_csv(f'2019/Pakistan_merged_data_points_2019_cleaned_{i}.csv')\n",
    "    data_2018_[i] = pd.read_csv(f'2018/Pakistan_merged_data_points_2018_cleaned_{i}.csv')\n",
    "    data_2017_[i] = pd.read_csv(f'2017/Pakistan_merged_data_points_2017_cleaned_{i}.csv')\n",
    "    data_2016_[i] = pd.read_csv(f'2016/Pakistan_merged_data_points_2016_cleaned_{i}.csv')\n",
    "    data_2015_[i] = pd.read_csv(f'2015/Pakistan_merged_data_points_2015_cleaned_{i}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98761313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will concate every file\n",
    "concate_data_ = {}\n",
    "\n",
    "for i in range(242):\n",
    "    concate_data_[i] = pd.concat([data_2024_[i], data_2023_[i], data_2022_[i], data_2021_[i], data_2020_[i], data_2019_[i], data_2018_[i], data_2017_[i],data_2016_[i], data_2015_[i]], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To make sure we are not duplicating any data\n",
    "# Compare all datasets Latitide and Longitude check if any of them have the same Latitude and Longitude\n",
    "# If they have the same Latitude and Longitude then print the data name and the Latitude and Longitude\n",
    "# Dictionary to store unique latitude-longitude pairs for each concate_data[i]\n",
    "lat_lon_sets = {}\n",
    "\n",
    "for i in range(242):  # 0 to 241\n",
    "    lat_lon_sets[i] = set(zip(concate_data_[i]['Latitude'], concate_data_[i]['Longitude']))\n",
    "\n",
    "# Flag to check if any common lat-lon is found\n",
    "found_common = False\n",
    "\n",
    "# Compare each file's lat-lon set with every other file\n",
    "for i in range(242):\n",
    "    for j in range(i + 1, 242):  # Avoid redundant comparisons\n",
    "        common_points = lat_lon_sets[i].intersection(lat_lon_sets[j])\n",
    "        if common_points:\n",
    "            found_common = True\n",
    "            print(f\"Common Latitude-Longitude points found between concate_data[{i}] and concate_data[{j}]:\")\n",
    "            for point in common_points:\n",
    "                print(point)\n",
    "            print(\"=\" * 50)\n",
    "\n",
    "# If no common lat-lon pairs are found\n",
    "if not found_common:\n",
    "    print(\"No same data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe116b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are checking if any data is missing\n",
    "print('concated data 0')\n",
    "print(concate_data_[0].Date.unique())\n",
    "print(concate_data_[0].Latitude.unique())\n",
    "print(concate_data_[0].Longitude.unique())\n",
    "print('/n concated data 1')\n",
    "print(concate_data_[1].Date.unique())\n",
    "print(concate_data_[1].Latitude.unique())\n",
    "print(concate_data_[1].Longitude.unique())\n",
    "print('/nconcated data 2')\n",
    "print(concate_data_[2].Date.unique())\n",
    "print(concate_data_[2].Latitude.unique())\n",
    "print(concate_data_[2].Longitude.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f46a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "concate_data_[1].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_df = pd.DataFrame([(i, lat, lon) for i, points in lat_lon_sets.items() for lat, lon in points], \n",
    "                          columns=['File_Index', 'Latitude', 'Longitude'])\n",
    "lat_lon_df.duplicated(subset=['Latitude', 'Longitude']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will save data for each point separately\n",
    "for i in range(242):\n",
    "    # Change file name as per your need\n",
    "    concate_data_[i].to_csv(f'Combined data/Pakistan_concated_data_{i}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
